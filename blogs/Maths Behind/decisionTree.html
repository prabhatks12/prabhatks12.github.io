<br />

<html>
<head><title>Maths behind Decision Tree</title>

<style>
	.box {
		  background-color: #E8E8E8;
		  width: 90%;
		  border: 2px solid black;
		  margin: 20px;
		  padding-top: 20px;
		  padding-bottom: 20px;
		  padding-left: 10px;
		  padding-right: 10px;
		}
		h3{
			font-weight: bold;
			color: red;
		}
		h4{
			font-weight: bold;
			color: blue;	
		}

	table, th, td {
		border: 1px solid black;
  		border-collapse: collapse;
  		padding: 10px;
	}

</style>
</head>

<body>
	<br>
	<h3>Introduction</h3>

	<p>
	Today we are going to learn about decision tree classifier. This blog is to focus more about the maths behind it rather than its implementation.
	
	The goal while creating a decision tree is to achieve maximum accuracy while maintaining the shortest 
	depth. The higher the depth, the higher is the chances of overfitting. <br /><br />

	For example: Let's think that the we have a target varialbe which is 'IceMelted' which tells whether an ice cube is melted or not based on many features including 'Temperature' and 'SizeIceCube' which stores the size of icecube. <br /><br />

	<table>
		<tr>
			<th>Temperature</th> <th>SizeIceCube</th> <th>IceMelted</th> <th>Others</th>
		</tr>

		<tr> <td>High</td> <td>Big</td>   <td>No</td>  <td>...</td> </tr>
		<tr> <td>Low</td>  <td>Big</td>   <td>No</td>  <td>...</td> </tr>
		<tr> <td>High</td> <td>Small</td> <td>Yes</td> <td>...</td> </tr>
		<tr> <td>Low</td>  <td>Small</td> <td>No</td>  <td>...</td> </tr>
		<tr> <td>...</td>  <td>...</td> <td>...</td>  <td>...</td> </tr>
		
	</table>
	
	
	

	<br />
	Let's find out in what order the features are going to be arranged in the decision tree. Following are some algorithms: <br /><br />

	
	
	<h3>Gini Impurity and Gain</h3>

	In most of the datasets, it is not the case that only one feature can clearly classify the target. The features are impure as in they are not fully capable of correctly classifying the target by themselves. Gini impurity is a measure of how much the can classify correctly. For example, It is not 100% guaranteed that 'High' 'Temperature' will melt the ice in the above dataset. If the size is big then it might not have melted.<br /><br />


	Formula for gini impurity in a leaf node:<br /><br />
	
	<div class="box">	
        						 
		g(Leaf) =  Σ P(k) * (1 - P(k)) <br />

	</div>
	
	For two lables -&gt; a and b:

	<div class="box">

		g(Leaf) = P(a) * (1 - P(a)) + P(b) * (1 - P(b)) <br />
		&nbsp;&nbsp;&nbsp; = P(a) - P(a)<sup>2</sup> + P(b) - P(b)<sup>2</sup>	<br />
		&nbsp;&nbsp;&nbsp; = {P(a) + P(b)} - P(a)<sup>2</sup> - P(b)<sup>2</sup>	<br />
		&nbsp;&nbsp;&nbsp; = 1 - P(a)<sup>2</sup> - P(b)<sup>2</sup> 	<br />

    </div>
	<br />
    The probability P(k) represents the number of data/rows corresponding to a particular sub-class(or label) divided by the total number of data present for the class. k represents the label number. 
    Example: The probability of value 'Yes' in 'IceMelted' divide by the sum of data entries for both 'Yes' and 'No'.<br /><br />


    As we are trying to classify the target class with the help of a feature 'Temperature', the image will look like:<br /><br /><div class="separator" style="clear: both; text-align: center;">
  
    	<img src="Images/Gini_reference.png">
    </div><div><br /></div><div>

    So, for target class IceMelted we have labels are 'Yes' and 'No'. The probability will be :<br /><br />

    	<div class="box">
			P(Yes) = IceMelted(Yes) / {IceMelted(No) + IceMelted(Yes)}	<br /><br />
		</div>		

    	The Gini impurity for left Leaf =&gt; Temperature= 'High' will be:	<br /><br />
    		
        <div class="box">
 		g(Temperature='High') = P(Yes)*{1-P(Yes)} + P(No)*{1-P(No)}	<br /><br />
 		</div>
    	
    	The Gini impurity for right Leaf =&gt; Temperature='Low' will be:	<br /><br />

	    <div class="box">
    		g(Temperature='Low') = P(Yes)*{1-P(Yes)} + P(No)*{1-P(No)}	<br /><br />
    	</div>	
    	
    	For the attribute Temperature, we will have to find the weighted Gini impurity(or Gini Gain) or total Gini impurity:<br /><br />

    		Let P(Left) define the total number rows in the left leaf/ total number of left and right leaf rows. Thus,<br /><br />

	    <div class="box">
    		G(Temperature) = P(Left)*g(Temperature='High') +  P(right)*g(Temperature='Low')	<br /><br />
    	</div>
    		

    The lower the impurity the better that attribute classifies the target. Thus, the lowest impurity attribute will be the root node. <br /><br />

    The graph shown below indicates the G(P) vs P. The impurity is maximum when we are not sure that the data falls in class 0 or class 1 (Highest point of G(P)). If we know for sure than the value of impurity is 0 (P=[0,1]). For displaying G(P) for multiple classes, the graph will become multidimensional.<br /><br />

    <div class="separator" style="clear: both; text-align: center;">
  
    	<img src="Images/gini_impurity.png">
    </div><div><br /></div><div>

    <br /><br /><h3>Entropy and Information gain</h3>

	Kullback–Leibler divergence: It calculates how much one probability distribution is different from others. <br /><br />

    <div class="box">

	Formula: D(P||Q) = Σ P(x) log(P(x)/Q(x))

	</div>

	What we are trying to minimize:<br /><br />

	For calculating the entropy we are trying to do the same. We are trying to reduce the similarity between the probability distribution of classes. For example: for a leaf node, if the probability is the same for each label corresponding to the node attribute, then the leaf node is not classifying anything. <br /><br />

	So, if the above case happened, then: <br /><br />

	<div class="box">
	Q(Label1) = Q(Label2) = Q(Label3) = .... = Q(Label K) = 1/K 
	</div>

	where K is the total number of labels and Q is there probability. Thus the above equation changes to:<br /><br />

	<div class="box">
	D(P||Q) = Σ P(x) { log(P(x)/(1/K) } <br />
	&nbsp;&nbsp;&nbsp; = Σ P(x) { log(P(x) - log(1/K) }	<br />		
	&nbsp;&nbsp;&nbsp; = Σ { P(x)log(P(x)) + P(x)log(K) } <br />
	</div>
	<br />
	
	Here, log(K) will be a constant. Thus log(K) Σ P(x) = log(K)	(Sum of probabiity p=1)	<br /><br />

	So we are trying to maximise =&gt; Σ P(x)log(P(x)) or minimize =&gt; - Σ P(x)log(P(x)) , which is the negative entropy = H(Leaf) <br /><br />

	So for the same node diagram above, we can represent it with node weighted entropy or information gain:<br /><br />

	<div class="box">
	H(Temperature) : P(Left) * H(Leaf) + P(Right)*H(Leaf) <br />
	</div>

	<br />
	The split is axis-aligned (we can plot the dataset, apply to the algorithm, and check the results). This means it either horizontal or vertical or its combination. <br /><br />

	<h2>References and amazing sources</h2>

	1. <a href="https://www.youtube.com/watch?v=a3ioGSwfVpE "> Machine Learning Lecture 29 "Decision Trees / Regression Trees" -Cornell CS4780 SP17</a>. Author: Kilian Weinberger<br />
	2. <a href="https://bambielli.com/til/2017-10-29-gini-impurity/"> Gini Impurity (With Examples)</a>.Author: Brian Ambielli<br />
	3. <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback–Leibler divergence- Wikipedia</a>.<br />
	4. <a href="https://www.youtube.com/watch?v=7VeUPuFGJHk">StatQuest: Decision Trees</a>.Author: Josh Starmer<br />


	<h2>Image references</h2>

	1. Gini impurity graph: <a href="https://towardsdatascience.com/gini-impurity-measure-dbd3878ead33">Gini Impurity Measure – a simple explanation using python</a>.Author: Steven Loaiza



<div class="separator" style="clear: both; text-align: center;"><br /></div></div><div class="separator" style="clear: both; text-align: center;"><br /></div><div class="separator" style="clear: both; text-align: center;"><br /></div></p></body></html>