<!DOCTYPE html>
<html>
<head>
	<title>Data Mining and AI</title>
	<link rel="stylesheet" type="text/css" href="css/style.css">
	<meta name="viewport" content="width=device-width, initial-scale=0.7">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
</head>


<body>

<div class="col-12 row">

	<div class="col-1"> </div>

	<div class="col-10">

	<br><br>


	<!-- ================================ Maths section =============================== -->

	<h5>Mathematics section</h5>
		<br>

		<table>

			<!-- headers -->
		    <tr> <th>Sl No</th> <th>Topic</th> <th>Explanation </th> <th>Notes</th> </tr>

		    <!-- 1 -->
			<tr> <td> 1 </td> 
				 <td> <b>The Mean, Variance and Standard Deviation</b></td>
				 <td> 
					  <a href="https://www.youtube.com/watch?v=SzZ6GpcfoQY&ab_channel=StatQuestwithJoshStarmer"> Statistics Fundamentals: The Mean, Variance and Standard Deviation : StatQuest</a>	<br> 
				 </td>
				 <td> None </td>
		  	</tr>


		    <!-- 2 -->
			<tr> <td> 2 </td> 
				 <td> <b>Statistical distribution</b></td>
				 <td> 
					  <a href="https://www.youtube.com/watch?v=oI3hZJqXJuc&ab_channel=StatQuestwithJoshStarmer"> Statistical distribution : StatQuest</a><br> 
				 </td>
				 <td> None </td>
		  	</tr>


		    <!-- 3 -->
			<tr> <td> 3 </td> 
				 <td> <b>Normal distribution</b></td>
				 <td> 
					  <a href="https://www.youtube.com/watch?v=rzFX5NWojp0&ab_channel=StatQuestwithJoshStarmer"> Normal distribution : StatQuest</a><br> 
				 </td>
				 <td>( 1/σ(√2π) ) * e ^ − 1/2 *(x−µ)^2/σ^2 todo, write prop				 
				</td>
		  	</tr>


		    <!-- 4 -->
			<tr> <td> 4 </td> 
				 <td> <b>Binomial Distribution and Test</b></td>
				 <td> 
					  <a href="https://www.youtube.com/watch?v=J8jNoF-K8E8&ab_channel=StatQuestwithJoshStarmer"> Binomial Distribution : StatQuest</a><br> 
				 </td>
				 <td> None </td>
		  	</tr>


		    <!-- 5 -->
			<tr> <td> 5 </td> 
				 <td> <b>Bayes theorem</b></td>
				 <td> 
					  <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"> Bayes theorem : Wikipedia</a><br>
					  <a href="https://www.youtube.com/watch?v=ibINrxJLvlM&ab_channel=Dr.TreforBazett"> Youtube </a> 
				 </td>
				 <td> None </td>
			</tr>

			<!-- 6 -->
			<tr> <td> 6 </td> 
				 <td> <b>Bernoulli equation for naive bayes</b></td>
				 <td> 
					  <a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Bernoulli_na%C3%AFve_Bayes"> Bayes theorem : Wikipedia</a><br>
				 </td>
				 <td> None </td>
		  	</tr>
		  	
			<!-- 7 -->
			<tr> <td> 7 </td> 
				 <td> <b>Bias and Variance</b></td>
				 <td> 
					  <a href="https://www.youtube.com/watch?v=EuBBz3bI-aA&ab_channel=StatQuestwithJoshStarmer"> Bias and Variance : StatQuest</a><br>
				 </td>
				 <td> None </td>
		  	</tr>

		 </table>

	<br><br>
	 
	 <!-- ================================ Pre Processing  and Fundamental section ============================= -->

	<h5>Pre-processing and Fundamental section</h5>
		<br>

		<table>

			<!-- headers -->
		    <tr> <th>Sl No</th> <th>Topic</th> <th>Explanation </th> <th>Implementation</th> <th>Notes</th> </tr>

		    <!-- 1 -->
			<tr> <td> 1 </td> 
				 <td> <b>Cross validation</b></td>
				 <td> 				 	
				   Best link: in implementation, can see this youtube video too.
				   <a href="https://www.youtube.com/watch?v=fSytzGwwBVw&ab_channel=StatQuestwithJoshStarmer"> Cross validation </a> , required for gaussian <br>
				   Type 1. K fold , 2. 10 folds, 3. Leave one out
				 </td>
				 <td> Best link: <a href="https://machinelearningmastery.com/k-fold-cross-validation/">machine learning mastery</a> </td>
				 <td> Try if you can get more accuracy using this, but should maintian consistency in real world too. </td>
			 </tr>

			<!-- 2 -->
			<tr> <td> 2 </td> 
				 <td> <b>Confusion Matrix , Sensitivity and Specificity</b></td>
				 <td>StatQuest video: <a href="https://youtu.be/Kdsp6soqA7o"> Confusion Matrix </a><br>
					<a href="https://youtu.be/vP06aMoz4v8">Sensitivity and Specificity</a>
				 </td>
				 <td> Sklearn cinfusion matrix, implementation with graph: <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py">Sklearn documentation</a> <br>
				 <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">Sensitivity and Specificity: Wikipedia</a>
				 </td>
				 <td> Used for finding accuracy of models. See, true positive, false positive, true negative, false negative. <br> <b>Sensitivity (true positive rate)</b>: correctly identifying positive proportion -> TP/(TP + FN), <b>Specificity (true negative rate): </b> correctly identifying negative proportion -> TN/(TN + FP). Useful when you want to decide whether you want to choose True pos rate or True neg rate. Which one is more imp for your scenrio.Example, for life threatning diceases, FP is better than FN (ie patient doesnt have the dicease but its predicted that he has) <b> Please see video for matrix greater than 2x2, for calc of TP and FP</b>

				 </td>
			 </tr>
			 
			 <!-- 3 -->
			<tr> <td> 3 </td> 
				 <td> <b> ROC and AUC </b></td>
				 <td>StatQuest video: <a href="https://youtu.be/4jRBRDbJemM"> ROC and AUC </a><br></td>
				 <td> ROC and AUC: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html">Sklearn documentation of score</a> </td>
				 <td> Roc is <b>graph between TP rate(sensitivity) and FP rate(1- Specificity)</b>. See precision too from the video and also see why he has choosen TP rate as 1 in ROC graph (imp). And AUC is useful to compare ROC graphs fr 2 or more approaches. The more Area Under the Graph an approach has, the better it is.</td>
			 </tr>

		 </table>

	<br><br>	

	<!-- ================================ Regression section =============================== -->

	Linear regression, Logistic regression, Polynomial regression, Gradient Descent, Stepwise regression, Ridge regression, Lasso regression, ElasticNet regression


ridge implementation : https://www.geeksforgeeks.org/implementation-of-ridge-regression-from-scratch-using-python/?ref=lbp

gradient reg types : geeks for geeks 3 types

https://www.kaggle.com/residentmario/gradient-descent-with-linear-regression
https://towardsdatascience.com/linear-regression-using-gradient-descent-in-10-lines-of-code-642f995339c0

<h5>Regression section</h5>
		<br>

		<table>

			<!-- headers -->
		    <tr> <th>Sl No</th> <th>Topic</th> <th>Explanation </th> <th>Implementation</th> <th>Notes</th> </tr>

		    <!-- 1 -->
		    <tr> <td> 1 </td> 
				 <td> <b>Ridge Regression</b> </td>
				 <td><a href="https://youtu.be/Q81RR3yKn30">StatQuest : Regularization Part 1: Ridge (L2) Regression</a></td>
				 <td></td>
				 <td></td>
			</tr>	


	    </table> 

	 <br><br>


	<!-- ================================ Classifers section =============================== -->

	<h5>Classifiers section</h5>
		<br>

		<table>

			<!-- headers -->
		    <tr> <th>Sl No</th> <th>Topic</th> <th>Explanation </th> <th>Implementation</th> <th>Notes</th> </tr>

		    <!-- 1 -->
			<tr> <td> 1 </td> 
				 <td> <b>Naive Bayes</b> , many types, main 3</td>
				 <td> 				 
				 	Everything is based on probability (including prior) and how we can calculate it.
				 	According to the type of input data, we have: <br>
					
					1. For symbolic or text data: <a href="https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer"> Multinomial naive bayes : StatQuest </a> <br>					
					2. For numerical data, having range mean and std dev:<a href="https://www.youtube.com/watch?v=H3EjCKtlVog&ab_channel=StatQuestwithJoshStarmer"> Gaussian/Normal naive bayes : StatQuest</a> <br>										
					3. For binary data: <a href="https://www.youtube.com/watch?v=ZxbF0qDe-pw&ab_channel=5MinutesEngineering">Bernoulli Naive Bayes : 5MinutesEngineering</a><br>
					4. See rest and all above: <a href="https://scikit-learn.org/stable/modules/naive_bayes.html">Complement and Categorical</a> <br>
 				    <b>Imp</b>
 				    In the error section of guassian distibution, see moving the threshold based on cost for False pos and False neg (from lecture pg 7). 

				 </td>
				 <td> Read from the book </td>
				 <td>
					It assumes attributes are independent and equally affects the target. <br>Works good for missing values, ignore that probability. <br> Laplace correction : adding 1 for 0 probability.
					<b>Extra</b> Gaussian plot is based on normal distibution equation. <br> See mean std deviation, statistical and normal distibution, bernauli's thm <b> (maths section) </b> 
			     </td>
			</tr>

			<tr> <td> 1 </td> 
				 <td> <b>Bagging and bootstrapping</b> </td>
				 <td> 
				 bagging also allows replacement
				 bootstrapping allows dublication due to replacement (see lectures)
				 some examples might not be included
				 for large dataset,0.632 prob that data will be used, 0.368 prob it will not be used

				 <a href="https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&ab_channel=StatQuestwithJoshStarmer">Bootstrap dataset (a part of it)</a>

				 </td>
				 <td></td>
				 <td></td>
			</tr>	

			 
		 </table> 

	 <br><br>

	<!-- ================================ Neural network section =============================== -->



	<h5>Neural Network section</h5>
		<br>

		<table>

			<!-- headers -->
		    <tr> <th>Sl No</th> <th>Topic</th> <th>Explanation </th> <th>Implementation</th> <th>Notes</th> </tr>

			<tr> <td colspan="5"> <b>6. Neural Network</b> (from Book 1 Ch 10) 
			<br> <a href="https://en.wikipedia.org/wiki/Biological_neuron_model">See how it resembles to human brain </a> <br> Feed forward multi-layer perceptron with logistic or linear threshold units trained by backward error propagation
			<br>
			
			<a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">1. A Step by Step Backpropagation Example</a> <br>
			<a href="https://www.anotsorandomwalk.com/backpropagation-example-with-numbers-step-by-step/">
			2. Backpropagation example with numbers step by step</a><br>
			<a href="https://www.youtube.com/watch?v=CqOfi41LfDw&ab_channel=StatQuestwithJoshStarmer">3. Neural network series (Imp)</a><br>

			</td></tr>

			<!-- 1 -->
			<tr> <td> 1 </td> <td> <b> Feed forward Neuron</b></td>
				 <td> 				 	
					 1. Wikipedia : <a href="https://en.wikipedia.org/wiki/Artificial_neuron"> Artificial neuron, </a> see how we perform calculation from lecture slides and performig logistic at each step (week 7) <br>

					 2. Wikipedia : <a href="https://en.wikipedia.org/wiki/Activation_function">All activation functions, </a> important ones are logistic, relu, sigmoid <br>

					 3. Simplilearn : <a href="https://www.youtube.com/watch?v=odlgtjXduVg&ab_channel=Simplilearn"> Youtube video </a> 



				 </td>
				 <td> Read from the book </td>
				 <td></td>
			</tr>

			
		</table>

	<br>
	Exam:

	1. Backpropagation
	2. Feed foward neural net
	3. classification with anns

	<br>
	Book : Data Mining : Practical Machine Learning Tools and Techniques, Fourth Edition, Ian H. Witten	
	</div>

	<div class="col-1"> </div>

</div>

</body>
</html>
