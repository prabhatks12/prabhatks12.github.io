<!DOCTYPE html>
<html>
<head>
	<title>Portfolio</title>
	<link rel="stylesheet" type="text/css" href="css/style.css">
	<meta name="viewport" content="width=device-width, initial-scale=0.7">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
</head>


<body>


Jobs requirement

1. Junior Data Scientist

Data Science, Machine Learning, Statistical Modelling techniques that may include; Gradient Boosting / XGBoost, Neural Networks, Recommendation / Decision / Scoring Engines, Predictive Modelling, Linear/Logistic Regression, Attribution Modelling, Customer Segmentation, Unsupervised & Supervised Learning, Bayesian Statistics, Principal Component Analysis

2. Data Scientist: 

Python development experience, preferably in the AI and ML space,
Database (SQL or NoSQL) development experience, Git


3. Data scientist:

Exposure to Big Data platforms, such as Data bricks or Spark.
Experience with SQL essential; exposure to Azure or GCP preferred.

4. Data scientist:

Fluency in at least one of the following programming languages R, Python, Scala
Familiarity with the following data-related technologies Hadoop, Pig, Hive, Impala, SQL, Teradata, Oracle, SAS, MongoDB
High-level understanding of architecting cloud-based solutions with the following products AWS Redshift/RDS, S3, EC2, Lambda, EMR, SageMaker, DynamoDB, Cloudformation, Athena, Kinesis â€“ or equivalents in Azure or Google Cloud Platform


5. Data scientist:

Experience using Python, PySpark, R or SparkR programming languages through development environments including Databricks for developing supervised and unsupervised machine learning models, with a particular emphasis on Natural Language Processing.
Demonstrated experience in using data science preferably for economic research, working with Australian Labour Market data such as ABS Quarterly Labour Force Survey and Census.
Experience with proof-of-concept development, preferably in Power BI or Shiny.
Experience working with big, unstructured datasets. Knowledge of the Burning Glass dataset would be an advantage.

6. ML Engineer:

Experience working with Kubernetes, preferably with GCP, Docker, Git and Cloud Build
Experience with SQL essential; exposure to Big Query / Airflow preferred
Deep understanding of CI/CD patterns, best practices and tooling options
You have experience in building Infrastructure as a Code with Terraform
Languages: Python, REST API process
Exposure to MLOps, Data Science and Data Engineering teams preferred
Designing and improving GCP infrastructure solutions for Data Science Applications



Final things: <br>
1. Course on git and github <br>


</body>

</html>